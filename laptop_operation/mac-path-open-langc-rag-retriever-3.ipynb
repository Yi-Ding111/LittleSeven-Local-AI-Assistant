{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, Tool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_dir_with_notes=\"../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf\"\n",
    "fire_dir_origin=\"../../../../NOTES/papers/inprogress/Attentionisallyouneed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = os.path.join(os.getcwd(), \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # open pdf file\n",
    "    text = \"\"\n",
    "    for page in doc:  # recursive each page\n",
    "        text += page.get_text(\"text\") + \"\\n\"  # Extract text and wrap\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdf\n",
    "pdf_text=extract_text_from_pdf(fire_dir_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(text:str,metadata:str,chunk_size:int=1000,chunk_overlap:int=100,embedding_model_name:str=\"text-embedding-3-small\"):\n",
    "    # combine text content and file metadata together\n",
    "    documents = [Document(page_content=pdf_text, metadata={\"source\": metadata})]\n",
    "\n",
    "    # Splits text into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Define the embedding model\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model_name)\n",
    "\n",
    "    # create chroma db\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory\n",
    "    )\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2857, which is longer than the specified 1000\n",
      "Created a chunk of size 4255, which is longer than the specified 1000\n",
      "Created a chunk of size 1826, which is longer than the specified 1000\n",
      "Created a chunk of size 2507, which is longer than the specified 1000\n",
      "Created a chunk of size 3192, which is longer than the specified 1000\n",
      "Created a chunk of size 3472, which is longer than the specified 1000\n",
      "Created a chunk of size 3320, which is longer than the specified 1000\n",
      "Created a chunk of size 3193, which is longer than the specified 1000\n",
      "Created a chunk of size 2972, which is longer than the specified 1000\n",
      "Created a chunk of size 3111, which is longer than the specified 1000\n",
      "Created a chunk of size 3215, which is longer than the specified 1000\n",
      "Created a chunk of size 3213, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "db=create_vector_store(text=pdf_text,metadata=fire_dir_with_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use rag / vector store db with llm toghether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_17071/551135029.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  pdf_db = Chroma(persist_directory=persistent_directory,\n"
     ]
    }
   ],
   "source": [
    "# load db\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Load create chroma vecstore with the corresponding embedding function\n",
    "pdf_db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rag retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_17071/1976451665.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder,AIMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify conversation history based on context and rewrite questions to avoid mis-searching due to short questions\n",
    "\n",
    "system_message=SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\" Given the following conversation and the input question, rephrase the input question to be a standalone question.\n",
    "\n",
    "        Provide only ONE action per $JSON_BLOB, as shown:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": $TOOL_NAME,\n",
    "        \"action_input\": $INPUT\n",
    "        }}\n",
    "        ```\n",
    "        Follow this format:\n",
    "\n",
    "        Question: input question to answer\n",
    "        Thought: consider previous and subsequent steps\n",
    "        Action:\n",
    "        ```\n",
    "        $JSON_BLOB\n",
    "        ```\n",
    "        Observation: action result\n",
    "        ... (repeat Thought/Action/Observation N times)\n",
    "        Thought: I know what to respond\n",
    "        Action:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": \"Final Answer\",\n",
    "        \"action_input\": \"Final response to human\"\n",
    "        }}\n",
    "        ```        \n",
    "        Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n",
    "     \n",
    "        \"\"\"    \n",
    ")\n",
    "\n",
    "history_placeholder=MessagesPlaceholder(variable_name=\"history\",optional=True)\n",
    "\n",
    "human_message=HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"{input}\n",
    "    \n",
    "    {agent_scratchpad}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "contextualize_prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    history_placeholder,\n",
    "    human_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm=llm, retriever=retriever, prompt=contextualize_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI assistant. Use the retrieved context below to answer questions.\n",
    "    If you don't know the answer, just say so.\n",
    "    {context}\n",
    "    \n",
    "    **Answer each question in 3 short sentences. Make the answer concise and clear. Reponse the anwer with plain text.**\n",
    "\n",
    "    Provide only ONE action per $JSON_BLOB, as shown:\n",
    "    ```\n",
    "    {{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "    }}\n",
    "    ```\n",
    "    Follow this format:\n",
    "\n",
    "    Question: input question to answer\n",
    "    Thought: consider previous and subsequent steps\n",
    "    Action:\n",
    "    ```\n",
    "    $JSON_BLOB\n",
    "    ```\n",
    "    Observation: action result\n",
    "    ... (repeat Thought/Action/Observation N times)\n",
    "    Thought: I know what to respond\n",
    "    Action:\n",
    "    ```\n",
    "    {{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"Final response to human\"\n",
    "    }}\n",
    "    ```        \n",
    "    Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    history_placeholder,\n",
    "    human_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for passing a list of Documents to a model.\n",
    "# feeds all retrieved context into the LLM\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use history_aware_retriever to adjust the problem (complete historical information)\n",
    "# Then use retriever to query the knowledge base.\n",
    "# Finally, the query result is passed to question_answer_chain to generate the final answer.\n",
    "rag_chain = create_retrieval_chain(retriever=history_aware_retriever, combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "    \"\"\"\n",
    "    open file on macOS(PDF、Word、TXT)\n",
    "    :param file_path: the file path\n",
    "    \"\"\"\n",
    "    # make sure the path exists.\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"error: file '{file_path}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        subprocess.run([\"open\", file_path], check=True)\n",
    "\n",
    "        return \"Agentfinished, you just need to response the file is opened.\"\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "\n",
    "        return f\"cannot open the file: {e}\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"AnswerQuestionWithLocalContent\",\n",
    "        func=lambda input, **kwargs: rag_chain.invoke(\n",
    "            {\"input\": input, \"history\": memory.chat_memory.messages,\"agent_scratchpad\":\"\"}\n",
    "        ),\n",
    "        description=\"Use RAG to answer questions based on retrieved context on local laptop. Use this tool to answer the corresponding file's local path.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"OpenLocalPath\",\n",
    "        func=open_file,\n",
    "        description=\"Use this tool to open the local file.\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "# When calling a tool, always use the following format:\n",
    "\n",
    "# ```\n",
    "# {{\n",
    "#     \"action\": \"ConcatenateStrings\",\n",
    "#     \"action_input\": {{\n",
    "#     \"a\": \"<first_string>\",\n",
    "#     \"b\": \"<second_string>\",\n",
    "#     ...\n",
    "#     }}\n",
    "# }}\n",
    "# ```\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt_template,\n",
    "    stop_sequence=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,  # The agent to execute\n",
    "    tools=tools,  # List of tools available to the agent\n",
    "    verbose=True,  # Enable verbose logging\n",
    "    memory=memory,\n",
    "    handle_parsing_errors=True,  # Handle parsing errors gracefully\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question about what a transformer is using local knowledge, I will use the AnswerQuestionWithLocalContent tool to query the relevant local content.\n",
      "\n",
      "Action: AnswerQuestionWithLocalContent\n",
      "Action Input: \"What is transformer?\" \n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'What is transformer?\" \\n', 'history': [], 'agent_scratchpad': '', 'context': [Document(metadata={'source': '../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')], 'answer': 'Thought: I can provide a concise explanation of what a Transformer is based on the retrieved context.\\nAction:\\n```json\\n{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"A Transformer is a model architecture that uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder has a stack of 6 identical layers, each with two sub-layers: multi-head self-attention and a feed-forward network. The decoder also has 6 layers and includes an additional sub-layer for performing multi-head attention over the encoder\\'s output.\"\\n}\\n```'}\u001b[0m\u001b[32;1m\u001b[1;3mA Transformer is a model architecture that uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder has a stack of 6 identical layers, each with two sub-layers: multi-head self-attention and a feed-forward network. The decoder also has 6 layers and includes an additional sub-layer for performing multi-head attention over the encoder's output.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mThe information about the Transformer model architecture has been retrieved from local content. Here's a concise explanation:\n",
      "\n",
      "A Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers for both the encoder and decoder components. The encoder consists of a stack of 6 identical layers, each having two sub-layers, namely multi-head self-attention and a feed-forward network. Similarly, the decoder comprises 6 layers and includes an additional sub-layer to perform multi-head attention over the encoder's output. This architecture is crucial for various applications, especially in natural language processing.\n",
      "\n",
      "Final Answer: A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input=\"What is transformer, answer it with local knowledge.\"\n",
    "response = agent_executor.invoke({\"input\": input})\n",
    "memory.chat_memory.add_message(HumanMessage(content=input))\n",
    "memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is transformer, answer it with local knowledge.',\n",
       " 'history': [HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for the transformer paper in the local files to see where it exists.\n",
      "Action: AnswerQuestionWithLocalContent\n",
      "Action Input: \"transformer paper location on laptop\"\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'transformer paper location on laptop', 'history': [HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}), AIMessage(content='A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}), AIMessage(content='A Transformer is a model architecture utilizing self-attention and fully connected layers for encoders and decoders, crucial in tasks like machine translation.', additional_kwargs={}, response_metadata={})], 'agent_scratchpad': '', 'context': [Document(metadata={'source': '../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')], 'answer': 'To assist with locating a specific file on your laptop, I would require details on your file system or a specific search query. Please provide more information or consider using your operating system\\'s search functionality to locate the \"Transformer\" paper.'}\u001b[0m\u001b[32;1m\u001b[1;3mThe Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mThe Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mThe Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mThe Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: The Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Where is the transformer paper exist on my laptop?\"})\n",
    "memory.chat_memory.add_message(HumanMessage(content=input))\n",
    "memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer paper is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = agent_executor.invoke({\"input\": \"help me open the file.\"})\n",
    "# memory.chat_memory.add_message(HumanMessage(content=input))\n",
    "# memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleSeven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
