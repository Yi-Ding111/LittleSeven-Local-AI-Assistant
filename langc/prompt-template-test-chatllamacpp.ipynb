{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.llamacpp import ChatLlamaCpp\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../littleSeven/')\n",
    "from common.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M2 Max) - 21845 MiB free\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../model_gguf/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:  CPU_AARCH64 model buffer size =  3474.00 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3647.87 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_4x8\n",
      "....\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 32\n",
      "llama_new_context_with_model: n_ubatch      = 8\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x136fd5c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x136fd5e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x127b43f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1275ee050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1272b77d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x1272b7a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x127a1f9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x127b437b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x127b3d770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1272b7ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x127b447f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x1272b80d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x127a20020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x127b45060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x16004ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x127a21340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x127b45830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x127b46030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x1600503b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x1272b9dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x160051980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x136c24ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x1272bacd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x127b46a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x127a22110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1272bb020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127a21960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x127a22ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127b47270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x127b47780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127b47d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x127b48290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x127b487a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1272bb3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127b48dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1272bbb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127b49370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1272bbfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x160050aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1272bc4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1272bc9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1272bcfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1272bd4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1272bdb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1272bdfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1272be600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1272beae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127b49d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1272bf060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x160051100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1272bf5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x160051330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1272bfe70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x127b49f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x127a23470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x127b4a4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127a23910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127a23ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127a244d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x160053230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127a24850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1600537e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127a24dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127a25350 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127b4aa40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127b4afc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x160053d60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127b4b540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127a258d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1272c00d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127a25ea0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127b4ba90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127a26450 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127b4c060 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127a26a20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127a26ff0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127a275c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1272c0680 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1272c0c50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127a27b90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127a28160 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1272c1220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127a28a70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127a28ff0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127a295c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1272c17f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1272c1dc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1272c2370 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1272c28f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1272c2ec0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x160054310 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127b4c630 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1272c3470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1600548e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1272c3a20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136d0d5d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127b4cc50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1272c3ff0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127b4cfa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1272c45c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x160054eb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127b4d4b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x160055480 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127a29b70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127b4da80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x160055a50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x160056020 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127a2a120 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1272c4b40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127a2a6a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127a2ac20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127a2b100 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1272c50c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127a2b6d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127b4e050 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127a2bc80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127a2c230 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127b4e620 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127b4eee0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1272c5690 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1272c5c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1272c6230 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1272c6830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127a2c830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1272c6e30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1272c7430 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127b4f4e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127b4fa60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1272c7a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127b50060 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127b50630 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127b50c30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127b51200 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127b51800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127a2ce00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1272c7fb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127b51dd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1600565f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x160056bf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1600571f0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1272c85b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x160057770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127a2d400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1272c8b30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1272c9040 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127a2d910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127a2de90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1272c95c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127b52350 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1272c9b40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x160057cf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127a2e410 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1272ca0c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127b52860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127a2e990 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1272ca670 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127a2ef40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1600582c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127a2f510 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136d092e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127b52e50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127b53400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127b539b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1272cac40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1600588c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127b53f80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127b54580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x160058e90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1272cb240 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127a2fb10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127b54b50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1272cb840 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1272cbdf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x160059490 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1272cc3f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127b55150 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127a30110 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1272cc9f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127b55720 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x160059a90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1272ccfa0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x16005a030 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127a30690 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1272cd5a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1272cdb50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x127a30c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x127b55ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x127a31190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1272ce0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1272ce6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x1272cebf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127b56300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x16005a650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127a31790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1272cf220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x1272cf7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1272cfcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127b56860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127b56e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1272d0190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127a31ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127a322c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127a32b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1272d0880 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127b57470 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127a33140 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127b57a40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1272d0e50 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x16005abd0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127a33740 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127a33cf0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127b58040 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1272d1420 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127b58640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x16005b1a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127a342d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127b58bf0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127b591f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1272d1a20 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x16005b3d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1272d2400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1272d2a00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1272d2f60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1272d3560 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1272d3b10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127b59770 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1272d4110 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1272d4690 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1272d4c90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1272d5260 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127a348b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127a34eb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127a354b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1272d5830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136c13550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127a35a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1272d5e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127a36060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127a36660 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x16005bc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1272d6440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x16005b600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127a36c10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127a37210 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1272d69e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x16005c7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127a377c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127a37dc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1272d6fe0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127a383c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1272d75e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x16005cd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x126604e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127b59cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127a38940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1272d7b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127a38e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1272d80b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1272d8600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127b5a200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127b5a780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127a393a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127a39920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x127b5ad00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x127b5b460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x127a3a0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x127a3a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x16005d4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1272d8b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x1272d8ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1272d9690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127a3ae80 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.11 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "model_path = expanduser(cfg.llm_gguf_model_path_2)\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_14199/950995765.py:8: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! The capital of France is Paris. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     992.78 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1796.43 ms /    52 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! The capital of France is Paris. \n"
     ]
    }
   ],
   "source": [
    "# create chat message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "# invoke model\n",
    "response = llm(messages)\n",
    "\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='  Of course! The capital of France is Paris. ', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-d0c107a8-e39c-421f-92f0-9d9ca69dec06-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the system message template\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Respond concisely to the user's query.\"\n",
    ")\n",
    "\n",
    "# define user message template\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# create ChatPromptTemplate (without history memory)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a helpful assistant. Respond concisely to the user's query.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a helpful assistant. Respond concisely to the user's query.\n",
      "human: Who is Stephen Curry in NBA?\n"
     ]
    }
   ],
   "source": [
    "# user input\n",
    "user_input = \"Who is Stephen Curry in NBA?\"\n",
    "\n",
    "# generate Prompt\n",
    "prompt_values = chat_prompt.format_prompt(user_input=user_input)\n",
    "\n",
    "# see Prompt\n",
    "messages = prompt_values.to_messages()\n",
    "for msg in messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 33 prefix-match hit, remaining 12 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stephen Curry is a professional basketball player who currently plays for the Golden State Warriors in the National Basketball Association (NBA). He is a shooting guard and point guard known for his incredible shooting ability and quick release. Curry has won numerous awards and accolades throughout his career, including two NBA championships, two NBA Finals MVPs, and four NBA scoring titles. He is widely considered one of the greatest shooters in NBA history and is often referred to as \"The Baby-Faced Assassin.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     992.78 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4842.55 ms /   122 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:   Stephen Curry is a professional basketball player who currently plays for the Golden State Warriors in the National Basketball Association (NBA). He is a shooting guard and point guard known for his incredible shooting ability and quick release. Curry has won numerous awards and accolades throughout his career, including two NBA championships, two NBA Finals MVPs, and four NBA scoring titles. He is widely considered one of the greatest shooters in NBA history and is often referred to as \"The Baby-Faced Assassin.\"\n"
     ]
    }
   ],
   "source": [
    "response = llm(messages)\n",
    "\n",
    "# print response\n",
    "print(\"Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a knowledgeable assistant specializing in geography.\n",
      "human: What is the largest desert in the world?\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"{role_description}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_values = chat_prompt.format_prompt(\n",
    "    role_description=\"You are a knowledgeable assistant specializing in geography.\",\n",
    "    user_input=\"What is the largest desert in the world?\"\n",
    ")\n",
    "\n",
    "\n",
    "for msg in prompt_values.to_messages():\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a knowledgeable assistant specializing in geography.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the largest desert in the world?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate message from template\n",
    "messages = prompt_values.to_messages()\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The largest desert in the world is the Sahara Desert, which covers much of North Africa. It spans across several countries including Algeria, Chad, Egypt, Libya, Mali, Mauritania, Morocco, Niger, and Tunisia. The Sahara Desert is approximately 9.4 million square kilometers (3.6 million square miles) in size, making it the largest hot desert in the world. It is known for its extreme heat and arid conditions, with some areas receiving less than 20 millimeters (0.8 inches) of rainfall per year. The Sahara is also home to several oases, which are fertile areas that support vegetation and wildlife despite the harsh desert environment."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     992.78 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7928.30 ms /   189 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:   The largest desert in the world is the Sahara Desert, which covers much of North Africa. It spans across several countries including Algeria, Chad, Egypt, Libya, Mali, Mauritania, Morocco, Niger, and Tunisia. The Sahara Desert is approximately 9.4 million square kilometers (3.6 million square miles) in size, making it the largest hot desert in the world. It is known for its extreme heat and arid conditions, with some areas receiving less than 20 millimeters (0.8 inches) of rainfall per year. The Sahara is also home to several oases, which are fertile areas that support vegetation and wildlife despite the harsh desert environment.\n"
     ]
    }
   ],
   "source": [
    "#  LLM\n",
    "response = llm(messages)\n",
    "\n",
    "# \n",
    "print(\"Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill the prompt template that could incoke tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# return local time\n",
    "def get_local_time(input: str = None) -> str:\n",
    "    \"\"\"return local time\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    try:\n",
    "        # get current local time\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"Current local time is: {current_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving local time: {str(e)}\"\n",
    "\n",
    "# tool defination\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalTime\",\n",
    "        func=get_local_time,\n",
    "        description=\"Use this tool to get the current local time. It does not require any input.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=lambda x: f\"Searching for: {x}\",\n",
    "        description=\"Use this tool for general information search.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool description\n",
    "tool_descriptions = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "# define Prompt Template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a smart assistant. Here are the tools you can use:\\n\"\n",
    "        \"{tool_descriptions}\\n\\n\"\n",
    "        \"When a user asks a question, decide if you need to use a tool. \"\n",
    "        \"If the question is about the current local time, use the `LocalTime` tool directly. \"\n",
    "        \"Respond with the tool's name and the input needed in the following format:\\n\"\n",
    "        \"action: <tool_name>\\n\"\n",
    "        \"action_input: <input>\\n\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a smart assistant. Here are the tools you can use:\n",
      "- LocalTime: Use this tool to get the current local time. It does not require any input.\n",
      "- Search: Use this tool for general information search.\n",
      "\n",
      "When a user asks a question, decide if you need to use a tool. If the question is about the current local time, use the `LocalTime` tool directly. Respond with the tool's name and the input needed in the following format:\n",
      "action: <tool_name>\n",
      "action_input: <input>\n",
      "\n",
      "human: What is the local time?\n"
     ]
    }
   ],
   "source": [
    "# user input\n",
    "user_input = \"What is the local time?\"\n",
    "\n",
    "# fill template\n",
    "prompt_values = chat_prompt.format_prompt(\n",
    "    tool_descriptions=tool_descriptions,\n",
    "    user_input=user_input\n",
    ")\n",
    "\n",
    "# generate message\n",
    "messages = prompt_values.to_messages()\n",
    "for msg in messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 76 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  action: LocalTime\n",
      "action_input: None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     992.78 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2550.88 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 'LocalTime' output: Current local time is: 2025-01-10 21:35:53\n"
     ]
    }
   ],
   "source": [
    "# invoke LLM\n",
    "response = llm(messages)\n",
    "\n",
    "# Parsing tool call logic\n",
    "if \"action:\" in response.content:\n",
    "    # Example parsing tool call\n",
    "    action, action_input = response.content.split(\"action_input:\", 1)\n",
    "    action_name = action.split(\"action:\")[1].strip()\n",
    "    action_input = action_input.strip()\n",
    "\n",
    "    # Find and call the corresponding tool\n",
    "    for tool in tools:\n",
    "        if tool.name == action_name:\n",
    "            tool_result = tool.func(action_input)\n",
    "            print(f\"Tool '{action_name}' output:\", tool_result)\n",
    "            break\n",
    "else:\n",
    "    print(\"Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='  action: LocalTime\\naction_input: None', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-d98710ac-159c-4fb9-8188-a3b9797387c0-0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleSeven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
