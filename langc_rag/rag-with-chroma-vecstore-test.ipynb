{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, Tool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_dir_with_notes=\"../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf\"\n",
    "fire_dir_origin=\"../../../../NOTES/papers/inprogress/Attentionisallyouneed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = os.path.join(os.getcwd(), \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # open pdf file\n",
    "    text = \"\"\n",
    "    for page in doc:  # recursive each page\n",
    "        text += page.get_text(\"text\") + \"\\n\"  # Extract text and wrap\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdf\n",
    "pdf_text=extract_text_from_pdf(fire_dir_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(text:str,metadata:str,chunk_size:int=1000,chunk_overlap:int=100,embedding_model_name:str=\"text-embedding-3-small\"):\n",
    "    # combine text content and file metadata together\n",
    "    documents = [Document(page_content=pdf_text, metadata={\"source\": metadata})]\n",
    "\n",
    "    # Splits text into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Define the embedding model\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model_name)\n",
    "\n",
    "    # create chroma db\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory\n",
    "    )\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2857, which is longer than the specified 1000\n",
      "Created a chunk of size 4255, which is longer than the specified 1000\n",
      "Created a chunk of size 1826, which is longer than the specified 1000\n",
      "Created a chunk of size 2507, which is longer than the specified 1000\n",
      "Created a chunk of size 3192, which is longer than the specified 1000\n",
      "Created a chunk of size 3472, which is longer than the specified 1000\n",
      "Created a chunk of size 3320, which is longer than the specified 1000\n",
      "Created a chunk of size 3193, which is longer than the specified 1000\n",
      "Created a chunk of size 2972, which is longer than the specified 1000\n",
      "Created a chunk of size 3111, which is longer than the specified 1000\n",
      "Created a chunk of size 3215, which is longer than the specified 1000\n",
      "Created a chunk of size 3213, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "db=create_vector_store(text=pdf_text,metadata=fire_dir_with_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use rag / vector store db with llm toghether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_22577/551135029.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  pdf_db = Chroma(persist_directory=persistent_directory,\n"
     ]
    }
   ],
   "source": [
    "# load db\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Load create chroma vecstore with the corresponding embedding function\n",
    "pdf_db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rag retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_22577/1976451665.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder,AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify conversation history based on context and rewrite questions to avoid mis-searching due to short questions\n",
    "\n",
    "system_message=SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\" Given the following conversation and the input question, rephrase the input question to be a standalone question.\n",
    "\n",
    "        Provide only ONE action per $JSON_BLOB, as shown:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": $TOOL_NAME,\n",
    "        \"action_input\": $INPUT\n",
    "        }}\n",
    "        ```\n",
    "        Follow this format:\n",
    "\n",
    "        Question: input question to answer\n",
    "        Thought: consider previous and subsequent steps\n",
    "        Action:\n",
    "        ```\n",
    "        $JSON_BLOB\n",
    "        ```\n",
    "        Observation: action result\n",
    "        ... (repeat Thought/Action/Observation N times)\n",
    "        Thought: I know what to respond\n",
    "        Action:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": \"Final Answer\",\n",
    "        \"action_input\": \"Final response to human\"\n",
    "        }}\n",
    "        ```        \n",
    "        Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n",
    "     \n",
    "        \"\"\"    \n",
    ")\n",
    "\n",
    "history_placeholder=MessagesPlaceholder(variable_name=\"history\",optional=True)\n",
    "\n",
    "human_message=HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"{input}\n",
    "    \n",
    "    {agent_scratchpad}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "contextualize_prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    history_placeholder,\n",
    "    human_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm=llm, retriever=retriever, prompt=contextualize_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI assistant. Use the retrieved context below to answer questions.\n",
    "    If you don't know the answer, just say so.\n",
    "    {context}\n",
    "    \n",
    "    **Answer each question in 3 short sentences. Make the answer concise and clear**\n",
    "\n",
    "    Provide only ONE action per $JSON_BLOB, as shown:\n",
    "    ```\n",
    "    {{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "    }}\n",
    "    ```\n",
    "    Follow this format:\n",
    "\n",
    "    Question: input question to answer\n",
    "    Thought: consider previous and subsequent steps\n",
    "    Action:\n",
    "    ```\n",
    "    $JSON_BLOB\n",
    "    ```\n",
    "    Observation: action result\n",
    "    ... (repeat Thought/Action/Observation N times)\n",
    "    Thought: I know what to respond\n",
    "    Action:\n",
    "    ```\n",
    "    {{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"Final response to human\"\n",
    "    }}\n",
    "    ```        \n",
    "    Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    history_placeholder,\n",
    "    human_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for passing a list of Documents to a model.\n",
    "# feeds all retrieved context into the LLM\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use history_aware_retriever to adjust the problem (complete historical information)\n",
    "# Then use retriever to query the knowledge base.\n",
    "# Finally, the query result is passed to question_answer_chain to generate the final answer.\n",
    "rag_chain = create_retrieval_chain(retriever=history_aware_retriever, combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Answer Question with local content\",\n",
    "        func=lambda input, **kwargs: rag_chain.invoke(\n",
    "            {\"input\": input, \"history\": memory.chat_memory.messages,\"agent_scratchpad\":\"\"}\n",
    "        ),\n",
    "        description=\"Use RAG to answer questions based on retrieved context on local laptop.\",\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\" Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "        {tools}\n",
    "        Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "        Valid \"action\" values: \"Final Answer\" or {tool_names}.\n",
    "        Provide only ONE action per $JSON_BLOB, as shown:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": $TOOL_NAME,\n",
    "        \"action_input\": $INPUT\n",
    "        }}\n",
    "        ```\n",
    "        Follow this format:\n",
    "\n",
    "        Question: input question to answer\n",
    "        Thought: consider previous and subsequent steps\n",
    "        Action:\n",
    "        ```\n",
    "        $JSON_BLOB\n",
    "        ```\n",
    "        Observation: action result\n",
    "        ... (repeat Thought/Action/Observation N times)\n",
    "        Thought: I know what to respond\n",
    "        Action:\n",
    "        ```\n",
    "        {{\n",
    "        \"action\": \"Final Answer\",\n",
    "        \"action_input\": \"Final response to human\"\n",
    "        }}\n",
    "        ```        \n",
    "        Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n",
    "     \n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "history_placeholder=MessagesPlaceholder(variable_name=\"history\",optional=True)\n",
    "\n",
    "scratchpad_placeholder = MessagesPlaceholder(variable_name=\"agent_scratchpad\", optional=True)\n",
    "\n",
    "human_message=HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"{input}\n",
    "    \n",
    "    {agent_scratchpad}\n",
    "    \n",
    "    (reminder to respond in a JSON blob no matter what)\"\"\"\n",
    ")\n",
    "\n",
    "assisstant_message=AIMessagePromptTemplate.from_template(\n",
    "    \"Understood! I will analyze your question type and provide a detailed response.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    history_placeholder,\n",
    "    human_message,\n",
    "    assisstant_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_structured_chat_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory,handle_parsing_errors=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's perform the calculation:\n",
      "\n",
      "15^2 means 15 multiplied by 15, which equals 225.\n",
      "\n",
      "Here is the response in a JSON blob as requested:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"15^2 equals 225.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is 15^2?', 'history': [HumanMessage(content='What is 15^2?', additional_kwargs={}, response_metadata={}), AIMessage(content='15^2 equals 225.', additional_kwargs={}, response_metadata={})], 'output': '15^2 equals 225.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What is 15^2?\",\n",
    "    \"history\": memory.chat_memory.messages,  \n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Answer Question with local content\",\n",
      "  \"action_input\": \"What is transformer?\"\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'What is transformer?', 'history': [HumanMessage(content='What is 15^2?', additional_kwargs={}, response_metadata={}), AIMessage(content='15^2 equals 225.', additional_kwargs={}, response_metadata={})], 'agent_scratchpad': '', 'context': [Document(metadata={'source': '../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')], 'answer': 'The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.'}\u001b[0m\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is transformer, answer it with local knowledge.', 'history': [HumanMessage(content='What is 15^2?', additional_kwargs={}, response_metadata={}), AIMessage(content='15^2 equals 225.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}), AIMessage(content='The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.', additional_kwargs={}, response_metadata={})], 'output': 'The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What is transformer, answer it with local knowledge.\",\n",
    "    \"history\": memory.chat_memory.messages,  \n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Answer Question with local content\",\n",
      "  \"action_input\": \"Where is the Transformer paper located on my laptop?\"\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'Where is the Transformer paper located on my laptop?', 'history': [HumanMessage(content='What is 15^2?', additional_kwargs={}, response_metadata={}), AIMessage(content='15^2 equals 225.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}), AIMessage(content='The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.', additional_kwargs={}, response_metadata={})], 'agent_scratchpad': '', 'context': [Document(metadata={'source': '../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')], 'answer': 'I can\\'t access or locate files on your laptop. You may want to search your device using relevant keywords such as \"Transformer paper\" or check in common directories where you store academic papers or documents.'}\u001b[0m\u001b[32;1m\u001b[1;3mCould not parse LLM output: The Transformer paper, titled \"Attention is All You Need,\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mCould not parse LLM output: The Transformer paper, titled \"Attention is All You Need,\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The Transformer paper, titled \\\"Attention is All You Need,\\\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Where is the paper exist on my laptop?', 'history': [HumanMessage(content='What is 15^2?', additional_kwargs={}, response_metadata={}), AIMessage(content='15^2 equals 225.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is transformer, answer it with local knowledge.', additional_kwargs={}, response_metadata={}), AIMessage(content='The Transformer is a model architecture that utilizes stacked self-attention and point-wise, fully connected layers in both its encoder and decoder components. The encoder consists of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, employing residual connections and layer normalization. Similarly, the decoder also stacks six layers, adding a third sub-layer for attending to encoder outputs and ensuring predictions depend only on preceding positions through masking.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where is the paper exist on my laptop?', additional_kwargs={}, response_metadata={}), AIMessage(content='The Transformer paper, titled \"Attention is All You Need,\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.', additional_kwargs={}, response_metadata={})], 'output': 'The Transformer paper, titled \"Attention is All You Need,\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\n",
    "    \"input\": \"Where is the paper exist on my laptop?\",\n",
    "    \"history\": memory.chat_memory.messages,  \n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer paper, titled \"Attention is All You Need,\" is located at the following path on your laptop: `../../../../NOTES/papers/inprogress/Attention-is-all-you-need.pdf`.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleSeven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
