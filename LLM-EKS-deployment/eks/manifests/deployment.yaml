apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ollama
  name: ollama-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama-llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama-llm
    spec:
      tolerations: # This tolerations rule allows the Pod to ignore the NoSchedule effect of nvidia.com/gpu and be scheduled to a GPU node. Tolerations allow the Pod to be scheduled to a node with the nvidia.com/gpu taint
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      # nodeSelector:  # Let the Pod run only on qualified Nodes. This means that the Pod can only be scheduled to Nodes with the label node-type=gpu. If the Node does not have this label, the Pod will not be scheduled to the Node.
      #   node-type: gpu
      containers:
        - image: ollama/ollama:latest
          imagePullPolicy: Always
          name: ollama-llm
          ports:
            - containerPort: 11434
          command: ["/bin/sh", "-c"] 
          args:
            - |
              ollama serve &  
              sleep 5  
              ollama pull deepseek-r1:8b  
              wait 
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama
          resources:
            limits:
              cpu: "4"
              memory: 16G
              nvidia.com/gpu: 1 
            requests:
              cpu: "2"
              memory: 8G
              nvidia.com/gpu: 1  
      volumes:
        - name: ollama-storage
          persistentVolumeClaim:
            claimName: ollama-pvc



