apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ollama
  name: ollama-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama-llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama-llm
    spec:
      tolerations: # è¿™ä¸ª tolerations è§„åˆ™å…è®¸ Pod å¿½ç•¥ nvidia.com/gpu çš„ NoSchedule å½±å“ï¼Œä»è€Œå¯ä»¥è°ƒåº¦åˆ° GPU èŠ‚ç‚¹ä¸Šã€‚ tolerations å…è®¸ Pod è°ƒåº¦åˆ° å¸¦æœ‰ nvidia.com/gpu æ±¡ç‚¹çš„èŠ‚ç‚¹
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      # nodeSelector:  # è®© Pod åªè¿è¡Œåœ¨ç¬¦åˆæ¡ä»¶çš„ Node ä¸Šã€‚è¡¨ç¤ºè¿™ä¸ª Pod åªèƒ½è°ƒåº¦åˆ° å¸¦æœ‰ node-type=gpu æ ‡ç­¾çš„ Node ä¸Šã€‚å¦‚æœ Node æ²¡æœ‰è¿™ä¸ªæ ‡ç­¾ï¼ŒPod å°±ä¸ä¼šè¢«è°ƒåº¦åˆ°è¯¥ Node ä¸Šã€‚
      #   node-type: gpu
      containers:
        - image: ollama/ollama:latest
          imagePullPolicy: Always
          name: ollama-llm
          ports:
            - containerPort: 11434
          command: ["/bin/sh", "-c"] # -cè¡¨ç¤ºæŠŠåé¢çš„å­—ç¬¦ä¸²ä¹‹å½¢æˆcml
          args:
            - |
              ollama serve &  
              sleep 5  
              ollama pull deepseek-r1:1.5b  
              wait 
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama
          resources:
            limits:
              cpu: "4"
              memory: 16G
              nvidia.com/gpu: 1  # ğŸš€ ç”³è¯· 1 ä¸ª GPU
            requests:
              cpu: "2"
              memory: 8G
              nvidia.com/gpu: 1  # ğŸš€ ç¡®ä¿è‡³å°‘åˆ†é… 1 ä¸ª GPU
      volumes:
        - name: ollama-storage
          persistentVolumeClaim:
            claimName: ollama-pvc



# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   namespace: ollama
#   name: ollama-deployment
# spec:
#   replicas: 2
#   selector:
#     matchLabels:
#       app.kubernetes.io/name: ollama-llm
#   template:
#     metadata:
#       labels:
#         app.kubernetes.io/name: ollama-llm
#     spec:
#       tolerations:
#         - key: "nvidia.com/gpu"
#           operator: "Exists"
#           effect: "NoSchedule"
#       nodeSelector:
#         node-type: gpu  # ä»…éœ€ä¸€ä¸ªæ ‡ç­¾
#       containers:
#         - image: ollama/ollama:latest
#           imagePullPolicy: Always
#           name: ollama-llm
#           ports:
#             - containerPort: 11434
#           command: ["/bin/sh", "-c"]
#           args:
#             - |
#               ollama serve &  
#               sleep 5  
#               ollama pull deepseek-r1:8b  
#               wait 
#           resources:  # å¿…é¡»å£°æ˜ GPU è¯·æ±‚
#             limits:
#               nvidia.com/gpu: 1
#             requests:
#               nvidia.com/gpu: 1
#           volumeMounts:
#             - name: ollama-storage
#               mountPath: /root/.ollama
#       volumes:
#         - name: ollama-storage
#           persistentVolumeClaim:
#             claimName: ollama-pvc