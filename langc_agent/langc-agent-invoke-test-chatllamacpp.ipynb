{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.llamacpp import ChatLlamaCpp\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../littleSeven/')\n",
    "from common.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent import Agent\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain_community.chat_models.llamacpp import ChatLlamaCpp\n",
    "from typing import List, Tuple, Any\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"tools\", \"history\", \"agent_scratchpad\"],\n",
    "#     template=(\n",
    "#         \"You are an assistant that can use tools to answer questions.\\n\"\n",
    "#         \"When deciding to use a tool, respond with:\\n\"\n",
    "#         \"Action: <tool_name>\\n\"\n",
    "#         \"Action Input: <tool_input>\\n\\n\"\n",
    "#         \"If you already have the answer, respond with:\\n\"\n",
    "#         \"Final Answer: <answer>\\n\\n\"\n",
    "#         \"Tools:\\n\"\n",
    "#         \"{tools}\\n\\n\"\n",
    "#         \"History:\\n\"\n",
    "#         \"{history}\\n\\n\"\n",
    "#         \"Scratchpad:\\n\"\n",
    "#         \"{agent_scratchpad}\\n\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"tools\", \"history\", \"agent_scratchpad\", \"input\"],\n",
    "#     template=(\n",
    "#         \"You are an assistant that can use tools to answer questions.\\n\"\n",
    "#         \"When deciding to use a tool, you MUST call the tool and wait for the result before answering.\\n\"\n",
    "#         \"Respond with:\\n\"\n",
    "#         \"Action: <tool_name>\\n\"\n",
    "#         \"Action Input: <tool_input>\\n\\n\"\n",
    "#         \"If you have used a tool and obtained the result, respond with:\\n\"\n",
    "#         \"Final Answer: <answer>\\n\\n\"\n",
    "#         \"Tools:\\n\"\n",
    "#         \"{tools}\\n\\n\"\n",
    "#         \"History:\\n\"\n",
    "#         \"{history}\\n\\n\"\n",
    "#         \"Scratchpad:\\n\"\n",
    "#         \"{agent_scratchpad}\\n\\n\"\n",
    "#         \"Question:\\n\"\n",
    "#         \"{input}\\n\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"tools\", \"history\", \"agent_scratchpad\", \"input\"],\n",
    "    template=(\n",
    "        \"You are an assistant that can use tools to answer questions.\\n\"\n",
    "        \"When deciding to use a tool, you MUST call the tool and wait for the result before answering.\\n\"\n",
    "        \"Respond with:\\n\"\n",
    "        \"Action: <tool_name>\\n\"\n",
    "        \"Action Input: <tool_input>\\n\\n\"\n",
    "        \"If you have used a tool and obtained the result, respond with:\\n\"\n",
    "        \"Final Answer: <answer>\\n\\n\"\n",
    "        \"Tools:\\n\"\n",
    "        \"{tools}\\n\\n\"\n",
    "        \"History:\\n\"\n",
    "        \"{history}\\n\\n\"\n",
    "        \"Scratchpad:\\n\"\n",
    "        \"{agent_scratchpad}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        \"{input}\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return time\n",
    "def get_local_time(input: str = None) -> str:\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"LocalTime tool called, returning: {current_time}\")\n",
    "    return f\"Current local time is: {current_time}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalTime\",\n",
    "        func=get_local_time,\n",
    "        description=\"Use this tool to get the current local time. No additional input is needed.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=lambda x: str(eval(x)),\n",
    "        description=\"Use this tool for mathematical calculations. Input should be a valid mathematical expression.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "# class CustomOutputParser(AgentOutputParser):\n",
    "#     def parse(self, text: str):\n",
    "#         print(f\"Parsing output: {text}\")\n",
    "#         if \"Final Answer:\" in text:\n",
    "#             final_answer = text.split(\"Final Answer:\")[1].strip()\n",
    "#             return AgentFinish({\"output\": final_answer}, text)\n",
    "#         elif \"Action:\" in text and \"Action Input:\" in text:\n",
    "#             action = text.split(\"Action:\")[1].split(\"\\n\")[0].strip()\n",
    "#             action_input = text.split(\"Action Input:\")[1].strip()\n",
    "#             print(f\"Parsed Action: {action}, Action Input: {action_input}\")\n",
    "#             return AgentAction(action, action_input, text)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Could not parse output: {text}\")\n",
    "\n",
    "# class CustomOutputParser(AgentOutputParser):\n",
    "#     def parse(self, text: str):\n",
    "#         print(f\"Parsing output: {text}\")\n",
    "#         if \"Final Answer:\" in text:\n",
    "#             final_answer = text.split(\"Final Answer:\")[1].strip()\n",
    "#             print(f\"Final Answer parsed: {final_answer}\")\n",
    "#             return AgentFinish({\"output\": final_answer}, text)\n",
    "#         elif \"Action:\" in text and \"Action Input:\" in text:\n",
    "#             action = text.split(\"Action:\")[1].split(\"\\n\")[0].strip()\n",
    "#             action_input = text.split(\"Action Input:\")[1].strip()\n",
    "#             print(f\"Parsed Action: {action}, Action Input: {action_input}\")\n",
    "#             return AgentAction(action, action_input, text)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Could not parse output: {text}\")\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        print(f\"Parsing output: {text}\")\n",
    "        if \"Final Answer:\" in text:\n",
    "            final_answer = text.split(\"Final Answer:\")[1].strip()\n",
    "            return AgentFinish({\"output\": final_answer}, text)\n",
    "        elif \"Action:\" in text and \"Action Input:\" in text:\n",
    "            action = text.split(\"Action:\")[1].split(\"\\n\")[0].strip()\n",
    "            action_input = text.split(\"Action Input:\")[1].strip()\n",
    "            print(f\"Parsed Action: {action}, Action Input: {action_input}\")\n",
    "            return AgentAction(action, action_input, text)\n",
    "        else:\n",
    "            raise ValueError(f\"Could not parse output: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M2 Max) - 21845 MiB free\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../model_gguf/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:  CPU_AARCH64 model buffer size =  3474.00 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3647.87 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_4x8\n",
      "....\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 32\n",
      "llama_new_context_with_model: n_ubatch      = 8\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x1179d7400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x10cc8e360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x116a30190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x10ccbd450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x10cc8e5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x116a307a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x10cc8eb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x1179a1e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x116d454e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1179d1a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x116a31720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x10cc8ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x10ccbcdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10ccbf050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x116a309d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x116a324d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x10ccbf820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x116d47340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x10ccc0010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x116a32ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x116a33ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x116a346d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x10ccc0830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x116a35030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x116a35830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1179d8a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ccc0d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x116a35fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116a36530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x116d45070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ccc1240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x116a36a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x116a36c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1200644b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1274b11b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120064e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1274b0bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1200650b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1200646e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120064910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120067610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d304da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120067b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ccc1790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ccc1d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120068110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d305100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1274afcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ccc2290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120068690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d3044d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1274af9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1274b22c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x120068be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1200691b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x120069700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120069c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12006a190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1274b1790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12006a7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12006ad00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12006b2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12006b7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120261a60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12006bd60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12006c2e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12006c830 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120260400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12006ce00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1202e4350 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1274b3470 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1202e4900 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1274aecc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1274b3790 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1202e4eb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1202e5460 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12006d3d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1274b3d40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12006d9a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1274b42f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1274b48c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12006df50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1202e5d00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d305330 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d305560 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1274b4e70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12006e500 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12006eab0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12006f060 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d307760 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1274b5440 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12006f630 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1274b5a10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1274b5fc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12006fc00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1274b6570 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1274b6b40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1200701d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1274b6e20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1200707a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1274b73d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1274b79a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120070dc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1274b7f70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d307d10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116a34b90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1274b8590 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120071370 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1200718f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1202e6280 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1274b8b10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d308290 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120071e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1274b90c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1202e6830 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d308860 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1202e6e00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1274b9670 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1274b9f10 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120072420 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1202e73d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1200729d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1202e79a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d308e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120072f80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120073580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1202e7fa0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1202e8570 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120073b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120074130 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d309430 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1274ba510 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120074730 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1202e8b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1274bab10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1202e9140 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120074ce0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1200752e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1274bb0e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1200758e0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1202e96c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1202e9cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120075e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1274bb660 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120076410 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120076920 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120076ea0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12026e2b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120077380 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1274bbbb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1274bc130 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d3099b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12026e800 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12026ed50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1274bc610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d309f30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1274bcc30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12026f320 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1274bd180 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d30a580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1274bd7a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120077950 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d30aad0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1274bdcf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1274be310 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d30b0f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1274be8b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120077f50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12026f8f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120078550 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120078b50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1274beeb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12026fef0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120079120 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1274bf480 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1274bfa50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1202704c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120270a90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120079720 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1274bfc80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120079d40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12007a340 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12007a910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1274bfeb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1274c0bc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x12007ae90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x12007b410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x116d47b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1274c1140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x116a35ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x116a36ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1179d8e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ccc24c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ccc2af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d30b760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x12007b9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x10d30bc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12007bdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12007bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x10d30c1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1179d9340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1274c19c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d30ca90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12007c310 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d30d0e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120271090 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1274c2000 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1274c2600 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d30d6e0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1274c2bd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d30dc80 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ccc2d20 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1179d9980 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d30e320 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120271660 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120271c60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d30e920 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12007d0e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120272260 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1274c34c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1179d9f80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116d45cc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12007d6e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1274c36f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d30ef60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1274c3920 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1179da580 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126306290 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d30f190 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1202726d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12007d910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1179da7b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12007db40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12007e860 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12007ee60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12007f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116a39610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120272900 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12007fa00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120273620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12007ffd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120273bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1200805e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1202741c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120080bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d30f3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1274c3b50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d30f5f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d30f820 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116d48890 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1200812b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x10d310ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x1202743f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120081900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1274c4850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120274bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d310d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120081e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d310f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1179da9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120082440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1200829c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116a39cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1274c4ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1274c55c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x1274c5dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x1274c67f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x1274c6dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1274c7310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x1274c7890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1274c7e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1179dac10 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.11 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "model_path = expanduser(cfg.llm_gguf_model_path_2)\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_17589/3307768691.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cunstruct input\n",
    "# tools_description = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "# print(tools_description)\n",
    "# # initial invoke，without history and intermediate steps\n",
    "# inputs = {\n",
    "#     \"tools\": tools_description,\n",
    "#     \"history\": \"\",\n",
    "#     \"agent_scratchpad\": \"\",\n",
    "#     \"input\": \"How are you?\"\n",
    "# }\n",
    "\n",
    "# # invoke LLMChain\n",
    "# output = llm_chain.predict(**inputs)\n",
    "\n",
    "\n",
    "# print(\"LLM Output:\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "class CustomAgent(Agent, BaseModel):\n",
    "    llm_chain: LLMChain = Field(...)\n",
    "    allowed_tools: List[str] = Field(...)\n",
    "    output_parser: CustomOutputParser = Field(default_factory=CustomOutputParser)\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        return \"custom-agent\"\n",
    "\n",
    "    @property\n",
    "    def observation_prefix(self) -> str:\n",
    "        return \"Observation:\"\n",
    "\n",
    "    @property\n",
    "    def llm_prefix(self) -> str:\n",
    "        return \"Thought:\"\n",
    "\n",
    "    def _get_default_output_parser(self):\n",
    "        return self.output_parser\n",
    "\n",
    "    @staticmethod\n",
    "    def create_prompt(tools: List[Tool], **kwargs: Any) -> str:\n",
    "        tool_descriptions = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "        return (\n",
    "            \"You are an assistant that can use the following tools:\\n\\n\"\n",
    "            f\"{tool_descriptions}\\n\\n\"\n",
    "            \"When you decide to use a tool, respond with:\\n\"\n",
    "            \"Action: <tool_name>\\n\"\n",
    "            \"Action Input: <tool_input>\\n\\n\"\n",
    "            \"If you already have the answer, respond with:\\n\"\n",
    "            \"Final Answer: <answer>\\n\"\n",
    "        )\n",
    "\n",
    "    def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) -> AgentAction:\n",
    "        thoughts = \"\".join(\n",
    "            [f\"{self.llm_prefix} {action.log}\\n{self.observation_prefix} {observation}\\n\" for action, observation in intermediate_steps]\n",
    "        )\n",
    "        inputs = {\"tools\": self.allowed_tools, \"history\": thoughts, **kwargs}\n",
    "        full_output = self.llm_chain.predict(**inputs)\n",
    "        return self.output_parser.parse(full_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_history = \"\"  # use null if without records\n",
    "default_agent_scratchpad = \"\"  # is null initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Agent and Executor\n",
    "custom_agent = CustomAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=custom_agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/b2jjkdb55_g_qzx3lsl5k59m0000gn/T/ipykernel_17589/788152752.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent_executor.run({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "  Action: LocalTime\n",
      "Action Input: None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4846.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5246.78 ms /   169 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing output:   Action: LocalTime\n",
      "Action Input: None\n",
      "Parsed Action: LocalTime, Action Input: None\n",
      "\u001b[32;1m\u001b[1;3m  Action: LocalTime\n",
      "Action Input: None\u001b[0mLocalTime tool called, returning: 2025-01-11 19:31:59\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mCurrent local time is: 2025-01-11 19:31:59\u001b[0m\n",
      "Thought:  Action: LocalTime"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 158 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: The current local time is 10:30 AM."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4846.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     892.76 ms /    24 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing output:   Action: LocalTime\n",
      "Final Answer: The current local time is 10:30 AM.\n",
      "\u001b[32;1m\u001b[1;3m  Action: LocalTime\n",
      "Final Answer: The current local time is 10:30 AM.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent Response: The current local time is 10:30 AM.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the local time?\"\n",
    "response = agent_executor.run({\n",
    "    \"tools\": \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "    \"history\": \"\",\n",
    "    \"agent_scratchpad\": \"\",\n",
    "    \"input\": \"What is the local time?\"\n",
    "})\n",
    "print(\"Agent Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleSeven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
